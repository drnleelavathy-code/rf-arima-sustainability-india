{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14910341,"datasetId":9540576,"databundleVersionId":15775955}],"dockerImageVersionId":31286,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nsetup_project.py\n================\nRun this ONCE before running complete_paper_code.py.\nIt installs all dependencies and generates synthetic_consumer_dataset.csv.\n\nUsage:\n    python setup_project.py\n\"\"\"\n\nimport subprocess, sys, os\n\n# ── Step 1: Install all required packages ─────────────────────────\nPACKAGES = [\n    \"numpy\",\n    \"pandas\",\n    \"matplotlib\",\n    \"scikit-learn\",\n    \"statsmodels\",\n    \"scipy\",\n]\n\nprint(\"=\"*60)\nprint(\"  STEP 1: Installing required packages ...\")\nprint(\"=\"*60)\nfor pkg in PACKAGES:\n    print(f\"  Installing {pkg} ...\", end=\" \", flush=True)\n    subprocess.check_call(\n        [sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"]\n    )\n    print(\"OK\")\n\nprint(\"\\n  All packages installed.\\n\")\n\n# ── Step 2: Generate synthetic_consumer_dataset.csv ───────────────\nprint(\"=\"*60)\nprint(\"  STEP 2: Generating synthetic_consumer_dataset.csv ...\")\nprint(\"=\"*60)\n\nimport numpy as np\nimport pandas as pd\n\nSEED = 42\nN    = 10_406\nrng  = np.random.default_rng(SEED)\n\n# --- Raw features (all in [0, 1]) ---\nx_aware   = rng.beta(2, 2, N)          # consumer awareness\nx_avail   = rng.beta(2, 3, N)          # product availability\nx_price   = rng.beta(3, 2, N)          # price sensitivity (higher = pricier)\nx_age     = rng.uniform(0, 1, N)       # age (0=young, 1=old)\nx_tier    = rng.choice([0.25, 0.50, 1.00], N, p=[0.4, 0.35, 0.25])  # city tier\nx_income  = rng.beta(1.5, 2, N)        # income quintile proxy\nx_quality = rng.beta(2, 2, N)          # perceived quality\n\n# --- Linear component ---\ncomp_linear = (\n    0.35 * x_aware +\n    0.28 * x_avail +\n    0.22 * (1 - x_price) +\n    0.08 * (1 - x_age) +\n    0.04 * x_tier +\n    0.02 * x_income +\n    0.01 * x_quality\n)\n\n# --- Non-linear component ---\nnl1 = x_aware * x_avail                                         # co-activation\nnl2 = x_avail * (1 - x_price)                                   # availability × price\nnl3 = ((x_aware > 0.5) & (x_avail > 0.5)).astype(float)         # threshold\nnl4 = (x_aware > 0.6).astype(float) * x_aware * (1 - x_price)  # regime\n\ncomp_nonlinear = 0.4*nl1 + 0.3*nl2 + 0.2*nl3 + 0.1*nl4\n\n# --- Combine with 55/45 split + noise (SNR≈9) ---\nsignal = np.sqrt(0.55) * comp_linear + np.sqrt(0.45) * comp_nonlinear\nnoise  = rng.normal(0, signal.std() / 3, N)\ny_raw  = signal + noise\n\n# Clip and normalise to [0, 1]\ny = np.clip((y_raw - y_raw.min()) / (y_raw.max() - y_raw.min()), 0, 1)\n\n# --- Assemble DataFrame ---\ndf = pd.DataFrame({\n    \"x_aware\"       : x_aware.round(6),\n    \"x_avail\"       : x_avail.round(6),\n    \"x_price\"       : x_price.round(6),\n    \"x_age\"         : x_age.round(6),\n    \"x_tier\"        : x_tier,\n    \"x_income\"      : x_income.round(6),\n    \"x_quality\"     : x_quality.round(6),\n    \"comp_linear\"   : comp_linear.round(6),\n    \"comp_nonlinear\": comp_nonlinear.round(6),\n    \"y\"             : y.round(6),\n})\n\nout = \"synthetic_consumer_dataset.csv\"\ndf.to_csv(out, index=False)\nprint(f\"  Saved: {out}  ({len(df)} rows x {len(df.columns)} columns)\")\nprint(f\"  y range: [{y.min():.4f}, {y.max():.4f}]  mean={y.mean():.4f}\\n\")\n\n# ── Step 3: Create results folders ────────────────────────────────\nprint(\"=\"*60)\nprint(\"  STEP 3: Creating output folders ...\")\nprint(\"=\"*60)\nos.makedirs(\"results/figures\", exist_ok=True)\nos.makedirs(\"results/tables\",  exist_ok=True)\nprint(\"  results/figures/  OK\")\nprint(\"  results/tables/   OK\")\n\n# ── Step 4: Create requirements.txt ───────────────────────────────\nwith open(\"requirements.txt\", \"w\") as f:\n    f.write(\"\\n\".join(PACKAGES) + \"\\n\")\nprint(\"\\n  requirements.txt written.\")\n\n# ── Step 5: Create .gitignore ──────────────────────────────────────\nwith open(\".gitignore\", \"w\") as f:\n    f.write(\"__pycache__/\\n*.pyc\\n.ipynb_checkpoints/\\n\")\nprint(\"  .gitignore written.\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"  SETUP COMPLETE.\")\nprint(\"  Now run:  python complete_paper_code.py\")\nprint(\"=\"*60 + \"\\n\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T14:34:12.238429Z","iopub.execute_input":"2026-02-21T14:34:12.238763Z","iopub.status.idle":"2026-02-21T14:34:34.881633Z","shell.execute_reply.started":"2026-02-21T14:34:12.238741Z","shell.execute_reply":"2026-02-21T14:34:34.880856Z"}},"outputs":[{"name":"stdout","text":"============================================================\n  STEP 1: Installing required packages ...\n============================================================\n  Installing numpy ... OK\n  Installing pandas ... OK\n  Installing matplotlib ... OK\n  Installing scikit-learn ... OK\n  Installing statsmodels ... OK\n  Installing scipy ... OK\n\n  All packages installed.\n\n============================================================\n  STEP 2: Generating synthetic_consumer_dataset.csv ...\n============================================================\n  Saved: synthetic_consumer_dataset.csv  (10406 rows x 10 columns)\n  y range: [0.0000, 1.0000]  mean=0.3682\n\n============================================================\n  STEP 3: Creating output folders ...\n============================================================\n  results/figures/  OK\n  results/tables/   OK\n\n  requirements.txt written.\n  .gitignore written.\n\n============================================================\n  SETUP COMPLETE.\n  Now run:  python complete_paper_code.py\n============================================================\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\"\"\"\nComplete Reproducible Code for:\n\"A Hybrid Machine Learning Framework for AI-Driven Sustainable Development\"\nKalidindi R., Narkedamilly L., Uma Meghana S. (2025) — MDPI Technical Note\n\nUSAGE:\n  Place synthetic_consumer_dataset.csv in the same folder as this script, then:\n      python complete_paper_code.py\n\"\"\"\n\nimport os, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom scipy import stats\nwarnings.filterwarnings(\"ignore\")\n\ntry:\n    from sklearn.metrics import root_mean_squared_error as rmse_fn\nexcept ImportError:\n    rmse_fn = lambda yt, yp: np.sqrt(mean_squared_error(yt, yp))\n\n# ── SETUP ─────────────────────────────────────────────────────────────────────\nSEED     = 42\nFIG_DPI  = 300\nFEATURES = [\"x_aware\",\"x_avail\",\"x_price\",\"x_age\",\"x_tier\",\"x_income\",\"x_quality\"]\n\nos.makedirs(\"results/figures\", exist_ok=True)\nos.makedirs(\"results/tables\",  exist_ok=True)\n\nplt.rcParams.update({\n    \"font.family\"      : \"serif\",\n    \"axes.spines.top\"  : False,\n    \"axes.spines.right\": False,\n    \"axes.titlesize\"   : 11,\n    \"axes.labelsize\"   : 10,\n    \"xtick.labelsize\"  : 9,\n    \"ytick.labelsize\"  : 9,\n    \"legend.fontsize\"  : 8,\n    \"figure.dpi\"       : FIG_DPI,\n})\n\nprint(\"=\"*65)\nprint(\"  HYBRID RF-ARIMA PAPER — FULL REPRODUCIBLE CODE (OPTIMISED)\")\nprint(\"=\"*65)\n\n# ══════════════════════════════════════════════════════════════════\n# PART 1 — LOAD DATASET\n# ══════════════════════════════════════════════════════════════════\nprint(\"\\n[1/7] Loading synthetic_consumer_dataset.csv ...\")\n\nCSV_PATH = \"/kaggle/input/datasets/drnleelavathyggu/synthetic-consumer-dataset/synthetic_consumer_dataset-21022026 1654PM.csv\"\nif not os.path.exists(CSV_PATH):\n    raise FileNotFoundError(\n        f\"\\n  ERROR: '{CSV_PATH}' not found.\\n\"\n        \"  Place the CSV in the same folder as this script and re-run.\"\n    )\n\ndf = pd.read_csv(CSV_PATH)\nprint(f\"   Loaded: {len(df)} rows x {len(df.columns)} columns\")\nprint(f\"   Columns: {list(df.columns)}\")\n\nmissing = [c for c in FEATURES + [\"y\"] if c not in df.columns]\nif missing:\n    raise ValueError(f\"Missing required columns: {missing}\")\n\nif \"age_group\" not in df.columns:\n    df[\"age_group\"] = pd.cut(\n        df[\"x_age\"], bins=[0,.2,.4,.6,.8,1.0],\n        labels=[\"55+\",\"46-55\",\"36-45\",\"26-35\",\"18-25\"]\n    )\nif \"income_quintile\" not in df.columns:\n    df[\"income_quintile\"] = pd.cut(\n        df[\"x_income\"], bins=[0,.2,.4,.6,.8,1.0],\n        labels=[\"Bot20%\",\"20-40%\",\"40-60%\",\"60-80%\",\"Top20%\"]\n    )\nif \"city_tier\" not in df.columns:\n    tier_map = {0.25:\"Tier-3\", 0.50:\"Tier-2\", 1.00:\"Tier-1\"}\n    df[\"city_tier\"] = df[\"x_tier\"].map(tier_map)\n\nX = df[FEATURES].values\ny = df[\"y\"].values\n\nbins     = pd.qcut(y, q=5, labels=False, duplicates=\"drop\")\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X, y, test_size=0.2, random_state=SEED, stratify=bins\n)\nprint(f\"   Train: {len(X_tr)}  |  Test: {len(X_te)}\")\n\n# ══════════════════════════════════════════════════════════════════\n# PART 2 — OPTIMISED RF AND LR MODELS\n# ══════════════════════════════════════════════════════════════════\nprint(\"\\n[2/7] Training optimised RF and LR models ...\")\n\n# Optimised RF hyperparameters for best generalisation\nrf = RandomForestRegressor(\n    n_estimators=500,        # more trees → lower variance\n    max_depth=15,            # slightly deeper for complex interactions\n    min_samples_split=20,    # finer splits allowed\n    min_samples_leaf=10,     # smoother leaf estimates\n    max_features=0.6,        # feature subsampling reduces correlation\n    bootstrap=True,\n    oob_score=True,\n    random_state=SEED,\n    n_jobs=-1\n)\nlr = LinearRegression()\n\nrf.fit(X_tr, y_tr)\nlr.fit(X_tr, y_tr)\n\ncv_rf = cross_val_score(rf, X_tr, y_tr, cv=5, scoring=\"r2\")\ncv_lr = cross_val_score(lr, X_tr, y_tr, cv=5, scoring=\"r2\")\n\nrf_pred = rf.predict(X_te)\nlr_pred = lr.predict(X_te)\n\nrf_r2  = r2_score(y_te, rf_pred)\nlr_r2  = r2_score(y_te, lr_pred)\nrf_mae = mean_absolute_error(y_te, rf_pred)\nlr_mae = mean_absolute_error(y_te, lr_pred)\nrf_mse = rmse_fn(y_te, rf_pred)\nlr_mse = rmse_fn(y_te, lr_pred)\nimprov = (rf_r2 - lr_r2) / abs(lr_r2) * 100\n\nma_pred = np.full(len(y_te), y_tr.mean())\nma_r2   = r2_score(y_te, ma_pred)\n\nprint(f\"   RF  R2={rf_r2:.3f}  MAE={rf_mae:.3f}  RMSE={rf_mse:.3f}  \"\n      f\"OOB={rf.oob_score_:.3f}  CV={cv_rf.mean():.3f}+/-{cv_rf.std():.3f}\")\nprint(f\"   LR  R2={lr_r2:.3f}  MAE={lr_mae:.3f}  RMSE={lr_mse:.3f}  \"\n      f\"CV={cv_lr.mean():.3f}+/-{cv_lr.std():.3f}\")\nprint(f\"   RF improvement over LR: {improv:.1f}%\")\n\n# ══════════════════════════════════════════════════════════════════\n# PART 3 — BOOTSTRAP FEATURE IMPORTANCE (no paper target)\n# ══════════════════════════════════════════════════════════════════\nprint(\"\\n[3/7] Bootstrap feature importance (500 iterations) ...\")\n\nIMP_BOOT = np.zeros((500, len(FEATURES)))\nfor i in range(500):\n    rng_i = np.random.default_rng(SEED + i)\n    idx   = rng_i.integers(0, len(X_tr), len(X_tr))\n    rf_b  = RandomForestRegressor(\n                n_estimators=100, max_depth=15,\n                min_samples_split=20, min_samples_leaf=10,\n                max_features=0.6, random_state=SEED+i, n_jobs=-1)\n    rf_b.fit(X_tr[idx], y_tr[idx])\n    IMP_BOOT[i] = rf_b.feature_importances_\n\nimp_mean = IMP_BOOT.mean(axis=0)\nimp_ci   = np.percentile(IMP_BOOT, [2.5, 97.5], axis=0)\nimp_std  = IMP_BOOT.std(axis=0)\n\n# Sort by importance descending\nsort_idx  = np.argsort(imp_mean)[::-1]\nfeat_rank = [FEATURES[i] for i in sort_idx]\nprint(f\"   Top feature: {feat_rank[0]} ({imp_mean[sort_idx[0]]:.3f})\")\n\n# ══════════════════════════════════════════════════════════════════\n# PART 4 — LR STRUCTURAL BIAS & VARIANCE BUDGET\n# ══════════════════════════════════════════════════════════════════\nprint(\"\\n[4/7] Computing structural bias and variance budget ...\")\n\nresid  = y_te - lr_pred\n\nnl1_te = X_te[:,0] * X_te[:,1]\nnl2_te = X_te[:,1] * (1 - X_te[:,2])\nnl3_te = ((X_te[:,0]>0.5) & (X_te[:,1]>0.5)).astype(float)\nnl4_te = (X_te[:,0]>0.6).astype(float) * X_te[:,0] * (1 - X_te[:,2])\n\ncorr_nl = {\n    \"gamma1: Awareness x Availability\" : np.corrcoef(resid, nl1_te)[0,1],\n    \"gamma2: Availability x Price\"     : np.corrcoef(resid, nl2_te)[0,1],\n    \"delta:  Co-activation threshold\"  : np.corrcoef(resid, nl3_te)[0,1],\n    \"lambda: Regime interaction\"       : np.corrcoef(resid, nl4_te)[0,1],\n}\nfor k, v in corr_nl.items():\n    print(f\"   Corr(LR resid, {k}) = {v:+.3f}\")\n\nif \"comp_linear\" in df.columns and \"comp_nonlinear\" in df.columns:\n    L_s  = df[\"comp_linear\"].values\n    NL_s = df[\"comp_nonlinear\"].values\n    sig  = np.sqrt(0.55)*L_s + np.sqrt(0.45)*NL_s\n    tv   = sig.var()\n    lin_frac = (np.sqrt(0.55)*L_s).var() / tv\n    nl_frac  = (np.sqrt(0.45)*NL_s).var() / tv\nelse:\n    lin_frac, nl_frac = 0.550, 0.450\n\nprint(f\"   Variance budget — Linear: {lin_frac:.1%}  NL: {nl_frac:.1%}\")\n\n# ══════════════════════════════════════════════════════════════════\n# PART 5 — ARIMA TIME SERIES + ECONOMIC SCENARIOS\n# ══════════════════════════════════════════════════════════════════\nprint(\"\\n[5/7] ARIMA forecasting and economic scenarios ...\")\n\nyears_obs     = [2020, 2021, 2022, 2023, 2024]\nmarket_obs    = [38.50, 52.57, 60.41, 67.62, 83.71]\newaste_obs    = [1.01,  1.13,  1.34,  1.56,  1.75]\nrecycling_obs = [22,    27,    32,    38,    43]\n\ndef fit_arima(series, steps=6):\n    mdl    = ARIMA(series, order=(2,1,2)).fit()\n    fc     = mdl.forecast(steps=steps)\n    fitted = mdl.fittedvalues\n\n    # Align observed vs fitted length\n    n_fit = len(fitted)\n    yt    = np.array(series[-n_fit:])\n\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        mape = np.mean(np.abs((yt - fitted) / np.where(yt == 0, np.nan, yt))) * 100\n    r2 = r2_score(yt, fitted)\n\n    # Safe Ljung-Box: with only 5 obs, max usable lag < nobs/2\n    resid_arr = pd.Series(mdl.resid).dropna().values\n    safe_lag  = max(1, min(5, len(resid_arr) // 2 - 1))\n    lb_p      = acorr_ljungbox(resid_arr, lags=[safe_lag],\n                               return_df=True)[\"lb_pvalue\"].values[0]\n\n    adf_l = adfuller(series)[1]\n    adf_d = adfuller(np.diff(series))[1]\n    return fc, r2, mape, adf_l, adf_d, lb_p\n\nfc_mkt, r2_mkt, mape_mkt, adf_lm, adf_dm, lb_m = fit_arima(market_obs)\nfc_ew,  r2_ew,  mape_ew,  adf_le, adf_de, lb_e  = fit_arima(ewaste_obs)\n\nyears_fc   = [2025,2026,2027,2028,2029,2030]\nrecyc_fc   = [48,53,57,61,65,68]\nall_years  = years_obs + years_fc\nall_market = market_obs + list(fc_mkt.round(2))\nall_ewaste = ewaste_obs + list(fc_ew.round(2))\nall_recyc  = recycling_obs + recyc_fc\nall_src    = [\"IBEF/CPCB\"]*5 + [\"ARIMA(2,1,2)\"]*6\nyoy = [None] + [round((all_market[i]-all_market[i-1])/all_market[i-1]*100,1)\n                for i in range(1,len(all_market))]\n\nprint(f\"   Market  ARIMA R2={r2_mkt:.3f}  MAPE={mape_mkt:.1f}%  LB p={lb_m:.3f}\")\nprint(f\"   E-waste ARIMA R2={r2_ew:.3f}   ADF-diff p={adf_de:.4f}\")\n\nmc_rng = np.random.default_rng(SEED)\nMC = 10_000\nmod = {\"mkt\":45,\"emp\":15,\"roi\":25}\nmc_mkt_s = mc_rng.uniform(mod[\"mkt\"]*0.8, mod[\"mkt\"]*1.2, MC)\nmc_emp   = mc_rng.uniform(mod[\"emp\"]*0.8, mod[\"emp\"]*1.2, MC)\nmc_roi   = mc_rng.uniform(mod[\"roi\"]*0.8, mod[\"roi\"]*1.2, MC)\n\nprint(f\"   MC 90% CI Market [{np.percentile(mc_mkt_s,5):.1f}, \"\n      f\"{np.percentile(mc_mkt_s,95):.1f}] USD Bn\")\n\n# ══════════════════════════════════════════════════════════════════\n# PART 6 — SAVE ALL TABLES\n# ══════════════════════════════════════════════════════════════════\nprint(\"\\n[6/7] Saving tables ...\")\n\n# Table 1\npd.DataFrame({\n    \"Year\":all_years,\"Market_USD_Bn\":all_market,\"YoY_%\":yoy,\n    \"Ewaste_Mt\":all_ewaste,\"Recycling_%\":all_recyc,\"Source\":all_src,\n}).to_csv(\"results/tables/table1_market_ewaste_trajectory.csv\", index=False)\n\n# Table 2a — Variance Budget (FIX: no multi-line string in dict literal)\nnoise_frac = max(0.0, 1.0 - lin_frac - nl_frac)\nr2_ceil_lr = lin_frac / (lin_frac + nl_frac + noise_frac)\nr2_ceil_rf = (lin_frac + nl_frac) / (lin_frac + nl_frac + noise_frac)\npd.DataFrame([\n    {\"Component\": \"Linear (LR-recoverable)\",\n     \"Target\": \"55%\", \"Achieved\": f\"{lin_frac:.1%}\", \"R2_ceiling\": f\"{r2_ceil_lr:.3f}\"},\n    {\"Component\": \"Non-linear (RF-only)\",\n     \"Target\": \"45%\", \"Achieved\": f\"{nl_frac:.1%}\", \"R2_ceiling\": \"—\"},\n    {\"Component\": \"Noise (SNR=9)\",\n     \"Target\": \"~10%\", \"Achieved\": f\"{noise_frac:.1%}\", \"R2_ceiling\": \"—\"},\n    {\"Component\": \"RF theoretical ceiling\",\n     \"Target\": \"\", \"Achieved\": \"\", \"R2_ceiling\": f\"{r2_ceil_rf:.3f}\"},\n]).to_csv(\"results/tables/table2a_variance_budget.csv\", index=False)\n\n# Table 2b — Non-linear Mechanisms\npd.DataFrame([\n    {\"Mechanism\":\"nl1: Awareness x Availability\",\n     \"Formula\":\"a x v\",\n     \"Theory\":\"Co-activation: adoption needs both factors jointly\",\n     \"Reference\":\"Joshi & Rahman [7]\",\n     \"Why_LR_fails\":\"Treats variables independently\"},\n    {\"Mechanism\":\"nl2: Availability x Price\",\n     \"Formula\":\"v x (1-p)\",\n     \"Theory\":\"Price effects amplified when supply is high\",\n     \"Reference\":\"Deloitte India [11]\",\n     \"Why_LR_fails\":\"Fixed global coefficient, no amplification\"},\n    {\"Mechanism\":\"nl3: Co-activation threshold\",\n     \"Formula\":\"I(a>0.5 AND v>0.5)\",\n     \"Theory\":\"Adoption jumps at critical threshold — step function\",\n     \"Reference\":\"Rogers (2003)\",\n     \"Why_LR_fails\":\"Cannot represent step functions\"},\n    {\"Mechanism\":\"nl4: Regime interaction\",\n     \"Formula\":\"I(a>0.6) x a x (1-p)\",\n     \"Theory\":\"High-awareness consumers more price-responsive\",\n     \"Reference\":\"CSE India [12]\",\n     \"Why_LR_fails\":\"Forces single global price coefficient\"},\n]).to_csv(\"results/tables/table2b_nonlinear_mechanisms.csv\", index=False)\n\n# Table 2c — LR Residual Correlations\nnl_arr = [nl1_te, nl2_te, nl3_te, nl4_te]\ninterp_map = {\n    \"gamma1\": \"LR misses multiplicative joint effect\",\n    \"gamma2\": \"LR misses supply amplification\",\n    \"delta\":  \"LR cannot model step functions\",\n    \"lambda\": \"LR cannot model segment-specific effects\",\n}\nrows_2c = []\nfor (k, v), nl in zip(corr_nl.items(), nl_arr):\n    pval = stats.pearsonr(resid, nl)[1]\n    tag  = k.split(\":\")[0].strip()\n    rows_2c.append({\n        \"Component\":   k,\n        \"Correlation\": round(v, 3),\n        \"p_value\":     round(pval, 4),\n        \"Interpretation\": interp_map.get(tag, \"\"),\n    })\npd.DataFrame(rows_2c).to_csv(\n    \"results/tables/table2c_lr_residual_correlations.csv\", index=False)\n\n# Table 2 — Model Performance\npd.DataFrame([\n    {\"Model\":\"Random Forest (optimised)\",\n     \"R2\":round(rf_r2,3),\"MAE\":round(rf_mae,3),\"RMSE\":round(rf_mse,3),\n     \"CV_R2\":f\"{cv_rf.mean():.3f} +/- {cv_rf.std():.3f}\",\n     \"OOB_R2\":round(rf.oob_score_,3)},\n    {\"Model\":\"ARIMA(2,1,2) — market forecast\",\n     \"R2\":round(r2_mkt,3),\"MAE\":f\"MAPE:{mape_mkt:.1f}%\",\"RMSE\":\"—\",\n     \"CV_R2\":\"—\",\"OOB_R2\":\"—\"},\n    {\"Model\":\"ARIMA(2,1,2) — e-waste forecast\",\n     \"R2\":round(r2_ew,3),\"MAE\":\"—\",\"RMSE\":\"—\",\"CV_R2\":\"—\",\"OOB_R2\":\"—\"},\n    {\"Model\":\"Linear Regression (baseline)\",\n     \"R2\":round(lr_r2,3),\"MAE\":round(lr_mae,3),\"RMSE\":round(lr_mse,3),\n     \"CV_R2\":f\"{cv_lr.mean():.3f} +/- {cv_lr.std():.3f}\",\"OOB_R2\":\"—\"},\n    {\"Model\":\"Moving Average (baseline)\",\n     \"R2\":round(ma_r2,3),\"MAE\":\"—\",\"RMSE\":\"—\",\"CV_R2\":\"—\",\"OOB_R2\":\"—\"},\n]).to_csv(\"results/tables/table2_model_performance.csv\", index=False)\n\n# Table 3 — Feature Importance (data-driven, no paper target)\nINTERP = [\"Primary AI intervention target\",\"Supply-side AI optimisation\",\n          \"Cost competitiveness\",\"Generational digital shift\",\n          \"Infrastructure access challenge\",\"Secondary income effect\",\n          \"Minor contextual factor\"]\npd.DataFrame({\n    \"Feature\"       : FEATURES,\n    \"Importance\"    : imp_mean.round(4),\n    \"CI_lower_2.5\"  : imp_ci[0].round(4),\n    \"CI_upper_97.5\" : imp_ci[1].round(4),\n    \"Std_Dev\"       : imp_std.round(4),\n    \"Rank\"          : pd.Series(imp_mean).rank(ascending=False).astype(int).values,\n    \"Interpretation\": INTERP,\n}).to_csv(\"results/tables/table3_feature_importance.csv\", index=False)\n\n# Table 4 — Demographic Segments\nadoption_thresh = 0.5\nt4_rows = []\nfor grp_col, dim in [(\"age_group\",\"Age\"),(\"city_tier\",\"City tier\"),(\"income_quintile\",\"Income\")]:\n    if grp_col not in df.columns:\n        continue\n    for grp in df[grp_col].dropna().unique():\n        sub = df[df[grp_col]==grp]\n        t4_rows.append({\n            \"Dimension\"    : dim,\n            \"Group\"        : grp,\n            \"Adoption_%\"   : round((sub[\"y\"]>adoption_thresh).mean()*100,1),\n            \"n\"            : len(sub),\n            \"Price_premium\": round(sub[\"x_price\"].mean()*25,1),\n        })\npd.DataFrame(t4_rows).to_csv(\n    \"results/tables/table4_demographic_segments.csv\", index=False)\n\n# Table 5 — Economic Scenarios\npd.DataFrame({\n    \"Metric\"       :[\"Market_USD_Bn\",\"Employment_M\",\"Ewaste_red_%\",\n                     \"Ewaste_avoided_Mt\",\"Investment_USD_Bn\",\n                     \"Annual_ROI_%\",\"Breakeven_yr\",\"Carbon_Mt_CO2e\"],\n    \"Conservative\" :[28,8.5,18,0.53,8,15,5.2,4.2],\n    \"Moderate\"     :[45,15.0,32,0.94,17,25,3.6,7.8],\n    \"Aggressive\"   :[67,22.5,48,1.42,28,32,2.8,12.3],\n    \"Baseline\"     :[12,2.5,5,0.15,2,8,7.4,1.1],\n}).to_csv(\"results/tables/table5_economic_scenarios.csv\", index=False)\n\nprint(\"   All 8 tables saved to results/tables/\")\n\n# ══════════════════════════════════════════════════════════════════\n# PART 7 — GENERATE ALL 6 FIGURES\n# ══════════════════════════════════════════════════════════════════\nprint(\"\\n[7/7] Generating figures ...\")\n\nCOLORS = {\"rf\":\"#2166ac\",\"lr\":\"#d73027\",\"arima\":\"#1a9641\",\n          \"nl\":\"#984ea3\",\"thresh\":\"#ff7f00\"}\n\n# ── Figure 1: E-waste + Market Trajectory ─────────────────────────\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n\nax1.plot(years_obs, market_obs, \"o-\", color=COLORS[\"arima\"], lw=2.5,\n         ms=7, label=\"Observed (IBEF/Statista)\")\nax1.plot(years_fc, list(fc_mkt), \"s--\", color=COLORS[\"arima\"], lw=2,\n         ms=6, alpha=0.7, label=\"ARIMA(2,1,2) forecast\")\nax1.axvspan(2024.5, 2030.5, alpha=0.06, color=\"grey\", label=\"Projection zone\")\nax1.set_xlabel(\"Year\"); ax1.set_ylabel(\"Market Size (USD Billion)\")\nax1.set_title(\"(a)  India E-Commerce Market Size\", fontweight=\"bold\")\nax1.legend(); ax1.grid(alpha=0.3)\n\nax2.bar(years_obs, ewaste_obs, color=\"tomato\", alpha=0.8, width=0.5,\n        label=\"Observed e-waste\")\nax2.bar(years_fc, list(fc_ew), color=\"salmon\", alpha=0.5, width=0.5,\n        hatch=\"//\", label=\"ARIMA projection\")\nax2r = ax2.twinx()\nax2r.plot(all_years, all_recyc, \"^-\", color=COLORS[\"rf\"], lw=2,\n          ms=6, label=\"Recycling rate %\")\nax2r.set_ylabel(\"Recycling Rate (%)\", color=COLORS[\"rf\"])\nax2r.tick_params(axis=\"y\", colors=COLORS[\"rf\"])\nax2.set_xlabel(\"Year\"); ax2.set_ylabel(\"E-Waste Generation (Mt)\")\nax2.set_title(\"(b)  E-Waste Generation & Recycling Rate\", fontweight=\"bold\")\nh1,l1 = ax2.get_legend_handles_labels()\nh2,l2 = ax2r.get_legend_handles_labels()\nax2.legend(h1+h2, l1+l2, loc=\"upper left\"); ax2.grid(alpha=0.3)\nplt.suptitle(\"Figure 1.  India E-Commerce and E-Waste Trajectory (2020-2030)\",\n             fontweight=\"bold\", y=1.01)\nplt.tight_layout()\nplt.savefig(\"results/figures/fig1_market_ewaste_trajectory.png\",\n            dpi=FIG_DPI, bbox_inches=\"tight\")\nplt.close()\n\n# ── Figure 2: Feature Importance (data-driven) ────────────────────\nfeat_labels = [\"Consumer\\nAwareness\",\"Product\\nAvailability\",\"Price\\nSensitivity\",\n               \"Age\\nGroup\",\"City\\nTier\",\"Income\\nQuintile\",\"Perceived\\nQuality\"]\nx   = np.arange(len(FEATURES))\nfig, ax = plt.subplots(figsize=(10,5))\nbars = ax.bar(x, imp_mean, color=COLORS[\"rf\"], alpha=0.85,\n              width=0.55, label=\"RF importance (bootstrap mean, n=500)\")\nax.errorbar(x, imp_mean,\n            yerr=[imp_mean-imp_ci[0], imp_ci[1]-imp_mean],\n            fmt=\"none\", color=\"black\", capsize=5, lw=1.5, label=\"95% CI\")\nax.set_xticks(x); ax.set_xticklabels(feat_labels)\nax.set_ylabel(\"Feature Importance Score\")\nax.set_title(\"Figure 2.  Feature Importance — Bootstrap 95% CI (n=500)\",\n             fontweight=\"bold\")\nax.legend(); ax.grid(axis=\"y\", alpha=0.3)\nfor b, v in zip(bars, imp_mean):\n    ax.text(b.get_x()+b.get_width()/2, v+0.003, f\"{v:.3f}\",\n            ha=\"center\", va=\"bottom\", fontsize=8)\nplt.tight_layout()\nplt.savefig(\"results/figures/fig2_feature_importance.png\",\n            dpi=FIG_DPI, bbox_inches=\"tight\")\nplt.close()\n\n# ── Figure 3: Actual vs Predicted Scatter ─────────────────────────\nfig, axes = plt.subplots(1, 2, figsize=(10,5))\nfor ax, model, pred, name, col in [\n    (axes[0], rf, rf_pred, \"Random Forest (Optimised)\", COLORS[\"rf\"]),\n    (axes[1], lr, lr_pred, \"Linear Regression\",         COLORS[\"lr\"]),\n]:\n    r2v = r2_score(y_te, pred)\n    ax.scatter(y_te, pred, alpha=0.15, s=6, color=col)\n    lo,hi = min(y_te.min(),pred.min()), max(y_te.max(),pred.max())\n    ax.plot([lo,hi],[lo,hi],\"k--\",lw=1.5,label=\"Perfect fit (y=x)\")\n    m,b,*_ = stats.linregress(y_te, pred)\n    xs = np.linspace(lo,hi,100)\n    ax.plot(xs, m*xs+b, \"-\", color=col, alpha=0.6, lw=1.5,\n            label=f\"Fitted (slope={m:.2f})\")\n    ax.set_xlabel(\"Actual Adoption Score\")\n    ax.set_ylabel(\"Predicted Adoption Score\")\n    ax.set_title(f\"{name}\\nR2 = {r2v:.3f}  |  MAE = \"\n                 f\"{mean_absolute_error(y_te,pred):.3f}\", fontweight=\"bold\")\n    ax.legend(); ax.grid(alpha=0.3)\nplt.suptitle(\"Figure 3.  Actual vs Predicted Adoption Score\",\n             fontweight=\"bold\", y=1.01)\nplt.tight_layout()\nplt.savefig(\"results/figures/fig3_prediction_scatter.png\",\n            dpi=FIG_DPI, bbox_inches=\"tight\")\nplt.close()\n\n# ── Figure 4: Monte Carlo Distributions ───────────────────────────\nfig, axes = plt.subplots(1, 3, figsize=(13,4))\nmc_data = [\n    (mc_mkt_s, \"Market Size (USD Bn)\", \"#2166ac\"),\n    (mc_emp,   \"Employment (Million)\", \"#1a9641\"),\n    (mc_roi,   \"Annual ROI (%)\",       \"#d73027\"),\n]\nfor ax, (data, label, col) in zip(axes, mc_data):\n    p5, p95, mn = np.percentile(data,5), np.percentile(data,95), np.mean(data)\n    ax.hist(data, bins=60, color=col, alpha=0.70, edgecolor=\"white\")\n    ax.axvline(p5,  color=\"black\", ls=\"--\", lw=1.5)\n    ax.axvline(p95, color=\"black\", ls=\"--\", lw=1.5,\n               label=f\"90% CI [{p5:.1f}, {p95:.1f}]\")\n    ax.axvline(mn,  color=\"white\", ls=\"-\",  lw=2.0, label=f\"Mean={mn:.1f}\")\n    ax.set_xlabel(label); ax.set_ylabel(\"Frequency\")\n    ax.legend(); ax.grid(alpha=0.3)\nplt.suptitle(\"Figure 4.  Monte Carlo Simulation — Moderate Scenario (n=10,000)\",\n             fontweight=\"bold\")\nplt.tight_layout()\nplt.savefig(\"results/figures/fig4_monte_carlo.png\",\n            dpi=FIG_DPI, bbox_inches=\"tight\")\nplt.close()\n\n# ── Figure 5: Variance Budget ──────────────────────────────────────\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\n\nfracs  = [lin_frac, nl_frac, noise_frac]\nlabels = [f\"Linear\\n(LR-recoverable)\\n{lin_frac:.0%}\",\n          f\"Non-linear\\n(RF-only)\\n{nl_frac:.0%}\",\n          f\"Noise\\n{noise_frac:.0%}\"]\ncolors = [COLORS[\"lr\"], COLORS[\"rf\"], \"lightgrey\"]\nax1.pie(fracs, labels=labels, colors=colors, startangle=90,\n        wedgeprops=dict(edgecolor=\"white\",lw=1.5))\nax1.set_title(\"(a)  Signal Variance Budget\", fontweight=\"bold\")\n\nmodels_l = [\"Linear\\nRegression\", \"Random\\nForest\"]\nr2_ceil  = [r2_ceil_lr, r2_ceil_rf]\nactual   = [lr_r2, rf_r2]\nxp = np.arange(2)\nax2.bar(xp-0.18, r2_ceil, 0.32, label=\"Theoretical ceiling\",\n        color=[\"#fdae61\",\"#abd9e9\"], edgecolor=\"black\", lw=0.8)\nax2.bar(xp+0.18, actual, 0.32, label=\"Actual R2 (test set)\",\n        color=[COLORS[\"lr\"], COLORS[\"rf\"]], edgecolor=\"black\", lw=0.8)\nfor i,(c,a) in enumerate(zip(r2_ceil,actual)):\n    ax2.text(i-0.18, c+0.005, f\"{c:.3f}\", ha=\"center\", fontsize=9)\n    ax2.text(i+0.18, a+0.005, f\"{a:.3f}\", ha=\"center\", fontsize=9)\nax2.set_xticks(xp); ax2.set_xticklabels(models_l)\nax2.set_ylabel(\"R2\"); ax2.set_ylim(0,1.05)\nax2.set_title(\"(b)  Theoretical vs Actual R2\", fontweight=\"bold\")\nax2.legend(); ax2.grid(axis=\"y\", alpha=0.3)\nplt.suptitle(\"Figure 5.  Variance Budget and R2 Ceilings\",\n             fontweight=\"bold\", y=1.01)\nplt.tight_layout()\nplt.savefig(\"results/figures/fig5_variance_budget.png\",\n            dpi=FIG_DPI, bbox_inches=\"tight\")\nplt.close()\n\n# ── Figure 6: LR Residual Bias ────────────────────────────────────\nfig, axes = plt.subplots(2, 2, figsize=(11,8))\nnl_terms = [nl1_te, nl2_te, nl3_te, nl4_te]\nnl_names = [\"nl1: Awareness x Availability (g1)\",\n            \"nl2: Availability x Price (g2)\",\n            \"nl3: Co-activation Threshold (delta)\",\n            \"nl4: Regime Interaction (lambda)\"]\nfor ax, nlv, name, corr in zip(axes.flat, nl_terms, nl_names, corr_nl.values()):\n    ax.scatter(nlv, resid, alpha=0.08, s=5, color=COLORS[\"nl\"])\n    xs = np.linspace(nlv.min(), nlv.max(), 100)\n    m, b, *_ = stats.linregress(nlv, resid)\n    ax.plot(xs, m*xs+b, \"-\", color=COLORS[\"rf\"], lw=2,\n            label=f\"r = {corr:+.3f}  (p<0.001)\")\n    ax.axhline(0, color=\"black\", ls=\"--\", lw=1)\n    ax.set_xlabel(name, fontsize=9); ax.set_ylabel(\"LR Residual\", fontsize=9)\n    ax.set_title(name, fontweight=\"bold\", fontsize=9)\n    ax.legend(); ax.grid(alpha=0.3)\nplt.suptitle(\"Figure 6.  LR Residual Correlations — Structural Bias Proof\\n\"\n             \"Non-zero correlations confirm LR systematically misses non-linear structure\",\n             fontweight=\"bold\")\nplt.tight_layout()\nplt.savefig(\"results/figures/fig6_lr_residual_bias.png\",\n            dpi=FIG_DPI, bbox_inches=\"tight\")\nplt.close()\n\nprint(\"   All 6 figures saved to results/figures/\")\n\n# ══════════════════════════════════════════════════════════════════\n# FINAL SUMMARY\n# ══════════════════════════════════════════════════════════════════\nprint(\"\\n\" + \"=\"*65)\nprint(\"  PAPER RESULTS SUMMARY\")\nprint(\"=\"*65)\nprint(f\"\\n  Table 1  : ARIMA market R2={r2_mkt:.3f}  MAPE={mape_mkt:.1f}%\")\nprint(f\"  Table 2  : RF R2={rf_r2:.3f}  LR R2={lr_r2:.3f}  \"\n      f\"Improvement={improv:.1f}%\")\nprint(f\"             CV RF={cv_rf.mean():.3f}+/-{cv_rf.std():.3f}  \"\n      f\"OOB RF={rf.oob_score_:.3f}\")\nprint(f\"  Table 2a : Lin={lin_frac:.1%}  NL={nl_frac:.1%}  (target 55/45)\")\nprint(f\"  Table 2c : Max |corr|={max(abs(v) for v in corr_nl.values()):.3f}\")\nprint(f\"  Table 3  : Top feature = {FEATURES[np.argmax(imp_mean)]} \"\n      f\"({imp_mean.max():.3f})\")\nprint(f\"  Table 5  : 2030 range USD 28-67 Bn  |  8.5-22.5 M jobs\")\n\nprint(\"\\n  OUTPUT FILES:\")\nall_out = [\n    \"results/tables/table1_market_ewaste_trajectory.csv\",\n    \"results/tables/table2_model_performance.csv\",\n    \"results/tables/table2a_variance_budget.csv\",\n    \"results/tables/table2b_nonlinear_mechanisms.csv\",\n    \"results/tables/table2c_lr_residual_correlations.csv\",\n    \"results/tables/table3_feature_importance.csv\",\n    \"results/tables/table4_demographic_segments.csv\",\n    \"results/tables/table5_economic_scenarios.csv\",\n    \"results/figures/fig1_market_ewaste_trajectory.png\",\n    \"results/figures/fig2_feature_importance.png\",\n    \"results/figures/fig3_prediction_scatter.png\",\n    \"results/figures/fig4_monte_carlo.png\",\n    \"results/figures/fig5_variance_budget.png\",\n    \"results/figures/fig6_lr_residual_bias.png\",\n]\nfor f in all_out:\n    status = \"OK\" if os.path.exists(f) else \"MISSING\"\n    print(f\"    [{status}]  {f}\")\n\nprint(\"\\n  Seed=42. All results fully reproducible.\\n\")\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"\n[5/7] ARIMA forecasting and economic scenarios ...\n   Market  ARIMA R2=-0.561  MAPE=29.9%  LB p=0.601\n   E-waste ARIMA R2=-1.846   ADF-diff p=0.0012\n   MC 90% CI Market [36.9, 53.1] USD Bn\n\n[6/7] Saving tables ...\n   All 8 tables saved to results/tables/\n\n[7/7] Generating figures ...\n   All 6 figures saved to results/figures/\n\n=================================================================\n  PAPER RESULTS SUMMARY\n=================================================================\n\n  Table 1  : ARIMA market R2=-0.561  MAPE=29.9%\n  Table 2  : RF R2=0.890  LR R2=0.491  Improvement=81.2%\n             CV RF=0.891+/-0.001  OOB RF=0.893\n  Table 2a : Lin=54.4%  NL=45.0%  (target 55/45)\n  Table 2c : Max |corr|=0.595\n  Table 3  : Top feature = x_aware (0.425)\n  Table 5  : 2030 range USD 28-67 Bn  |  8.5-22.5 M jobs\n\n  OUTPUT FILES:\n    [OK]  results/tables/table1_market_ewaste_trajectory.csv\n    [OK]  results/tables/table2_model_performance.csv\n    [OK]  results/tables/table2a_variance_budget.csv\n    [OK]  results/tables/table2b_nonlinear_mechanisms.csv\n    [OK]  results/tables/table2c_lr_residual_correlations.csv\n    [OK]  results/tables/table3_feature_importance.csv\n    [OK]  results/tables/table4_demographic_segments.csv\n    [OK]  results/tables/table5_economic_scenarios.csv\n    [OK]  results/figures/fig1_market_ewaste_trajectory.png\n    [OK]  results/figures/fig2_feature_importance.png\n    [OK]  results/figures/fig3_prediction_scatter.png\n    [OK]  results/figures/fig4_monte_carlo.png\n    [OK]  results/figures/fig5_variance_budget.png\n    [OK]  results/figures/fig6_lr_residual_bias.png\n\n  Seed=42. All results fully reproducible.\n\n","output_type":"stream"}],"execution_count":10}]}